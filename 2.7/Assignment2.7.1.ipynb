{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary libraries \n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import joblib \n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and loading data\n",
    "df = pd.read_csv('sensor.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# Divide the data into sets for training, testing, and validation.\n",
    "\n",
    "train_df = df[df['timestamp'] < '2018-07-01']\n",
    "validation_df = df[(df['timestamp'] >= '2018-07-01') & (df['timestamp'] < '2018-08-01')]\n",
    "test_df = df[df['timestamp'] >= '2018-08-01'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store these three portions into distinct CSV files.\n",
    "train_df.to_csv('train.csv', index=False)\n",
    "validation_df.to_csv('validation.csv', index=False)\n",
    "test_df.to_csv('test.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Considering this involves identifying anomalies in time-series data, a potential approach is to utilize an Isolation Forest. This algorithm, classified under unsupervised learning, is particularly effective for anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0             0\n",
      "timestamp              0\n",
      "sensor_00           6443\n",
      "sensor_01             64\n",
      "sensor_02              9\n",
      "sensor_03              9\n",
      "sensor_04              9\n",
      "sensor_05              9\n",
      "sensor_06           1002\n",
      "sensor_07           1655\n",
      "sensor_08           1365\n",
      "sensor_09            793\n",
      "sensor_10              9\n",
      "sensor_11              9\n",
      "sensor_12              9\n",
      "sensor_13              9\n",
      "sensor_14             16\n",
      "sensor_15         131040\n",
      "sensor_16             26\n",
      "sensor_17             41\n",
      "sensor_18             41\n",
      "sensor_19             11\n",
      "sensor_20             11\n",
      "sensor_21             11\n",
      "sensor_22             36\n",
      "sensor_23             11\n",
      "sensor_24             11\n",
      "sensor_25             31\n",
      "sensor_26             15\n",
      "sensor_27             11\n",
      "sensor_28             11\n",
      "sensor_29             67\n",
      "sensor_30            256\n",
      "sensor_31             11\n",
      "sensor_32             63\n",
      "sensor_33             11\n",
      "sensor_34             11\n",
      "sensor_35             11\n",
      "sensor_36             11\n",
      "sensor_37             11\n",
      "sensor_38             17\n",
      "sensor_39             17\n",
      "sensor_40             17\n",
      "sensor_41             17\n",
      "sensor_42             17\n",
      "sensor_43             17\n",
      "sensor_44             17\n",
      "sensor_45             17\n",
      "sensor_46             17\n",
      "sensor_47             17\n",
      "sensor_48             17\n",
      "sensor_49             17\n",
      "sensor_50             17\n",
      "sensor_51          15373\n",
      "machine_status         0\n",
      "dtype: int64\n",
      "Unnamed: 0    1.430968e+09\n",
      "sensor_00     1.763570e-01\n",
      "sensor_01     6.863599e+00\n",
      "sensor_02     1.031378e+01\n",
      "sensor_03     4.790741e+00\n",
      "sensor_04     1.969243e+04\n",
      "sensor_05     3.658947e+02\n",
      "sensor_06     4.603760e+00\n",
      "sensor_07     5.242063e+00\n",
      "sensor_08     4.112306e+00\n",
      "sensor_09     4.700365e+00\n",
      "sensor_10     1.468311e+02\n",
      "sensor_11     1.465509e+02\n",
      "sensor_12     1.008618e+02\n",
      "sensor_13     2.046737e+01\n",
      "sensor_14     1.927554e+04\n",
      "sensor_15              NaN\n",
      "sensor_16     2.406543e+04\n",
      "sensor_17     2.594099e+04\n",
      "sensor_18     9.053502e-01\n",
      "sensor_19     5.976489e+04\n",
      "sensor_20     1.559891e+04\n",
      "sensor_21     7.743383e+04\n",
      "sensor_22     3.357236e+04\n",
      "sensor_23     1.099610e+05\n",
      "sensor_24     4.995561e+04\n",
      "sensor_25     7.245958e+04\n",
      "sensor_26     8.187241e+04\n",
      "sensor_27     2.901546e+04\n",
      "sensor_28     1.390305e+05\n",
      "sensor_29     8.302756e+04\n",
      "sensor_30     5.317082e+04\n",
      "sensor_31     1.224035e+05\n",
      "sensor_32     8.891280e+04\n",
      "sensor_33     3.616286e+04\n",
      "sensor_34     7.440547e+03\n",
      "sensor_35     1.936449e+04\n",
      "sensor_36     9.567055e+04\n",
      "sensor_37     7.423475e+02\n",
      "sensor_38     1.250311e+02\n",
      "sensor_39     3.123532e+02\n",
      "sensor_40     4.170035e+02\n",
      "sensor_41     7.652588e+01\n",
      "sensor_42     1.507414e+02\n",
      "sensor_43     1.549692e+02\n",
      "sensor_44     5.004723e+01\n",
      "sensor_45     1.040200e+02\n",
      "sensor_46     1.786165e+02\n",
      "sensor_47     6.642486e+01\n",
      "sensor_48     3.262370e+03\n",
      "sensor_49     1.816546e+02\n",
      "sensor_50     3.249559e+03\n",
      "sensor_51     6.615391e+03\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hassa\\AppData\\Local\\Temp\\ipykernel_7728\\833404133.py:6: FutureWarning: The default value of numeric_only in DataFrame.var is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  print(df_train.var())\n"
     ]
    }
   ],
   "source": [
    "# Loading Train data \n",
    "df_train = pd.read_csv('train.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# Inspect data\n",
    "print(df_train.isna().sum())\n",
    "print(df_train.var())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dropping unnecessary columns\n",
    "\n",
    "df_train.drop(['Unnamed: 0', 'timestamp', 'machine_status'], axis=1, inplace=True)\n",
    "\n",
    "# Filling NaN values with mean\n",
    "df_train.fillna(df_train.mean(), inplace=True)\n",
    "\n",
    "\n",
    "df_train.drop('sensor_15', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensor_00    0\n",
      "sensor_01    0\n",
      "sensor_02    0\n",
      "sensor_03    0\n",
      "sensor_04    0\n",
      "sensor_05    0\n",
      "sensor_06    0\n",
      "sensor_07    0\n",
      "sensor_08    0\n",
      "sensor_09    0\n",
      "sensor_10    0\n",
      "sensor_11    0\n",
      "sensor_12    0\n",
      "sensor_13    0\n",
      "sensor_14    0\n",
      "sensor_16    0\n",
      "sensor_17    0\n",
      "sensor_18    0\n",
      "sensor_19    0\n",
      "sensor_20    0\n",
      "sensor_21    0\n",
      "sensor_22    0\n",
      "sensor_23    0\n",
      "sensor_24    0\n",
      "sensor_25    0\n",
      "sensor_26    0\n",
      "sensor_27    0\n",
      "sensor_28    0\n",
      "sensor_29    0\n",
      "sensor_30    0\n",
      "sensor_31    0\n",
      "sensor_32    0\n",
      "sensor_33    0\n",
      "sensor_34    0\n",
      "sensor_35    0\n",
      "sensor_36    0\n",
      "sensor_37    0\n",
      "sensor_38    0\n",
      "sensor_39    0\n",
      "sensor_40    0\n",
      "sensor_41    0\n",
      "sensor_42    0\n",
      "sensor_43    0\n",
      "sensor_44    0\n",
      "sensor_45    0\n",
      "sensor_46    0\n",
      "sensor_47    0\n",
      "sensor_48    0\n",
      "sensor_49    0\n",
      "sensor_50    0\n",
      "sensor_51    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check Nan values\n",
    "print(df_train.isna().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML: Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hassa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df_train), columns=df_train.columns)\n",
    "\n",
    "# Defining model\n",
    "model = IsolationForest(contamination=0.05)\n",
    "\n",
    "# Fitting model\n",
    "model.fit(df_scaled)\n",
    "\n",
    "# Utilize the trained model on the dataset\n",
    "scores = model.decision_function(df_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.joblib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Storing model and scaler\n",
    "joblib.dump(model, 'model.joblib')\n",
    "joblib.dump(scaler, 'scaler.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileProcessor(FileSystemEventHandler):\n",
    "\n",
    "    '''\n",
    "    This class manages newly generated data files situated in the input directory.\n",
    "\n",
    "    It reacts to events when new files are created by reading the file, \n",
    "    Transforming its data, utilizing a pre-trained model for predictions, and subsequently storing the results.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, configuration):\n",
    "\n",
    "        '''\n",
    "        Sets up the FileProcessor using the given configuration.\n",
    "\n",
    "        Args:\n",
    "            configuration (dictionary): A dictionary that includes paths to the model, scaler, logs, and other parameters.\n",
    "        '''\n",
    "\n",
    "        self.configuration = configuration\n",
    "        # Loading the pre-trained model from the file\n",
    "        self.loaded_model = joblib.load(configuration['model_path'])\n",
    "        # Loading the data scaler from the file\n",
    "        self.data_scaler = joblib.load(configuration['scaler_path'])\n",
    "        # Initializing the logging\n",
    "        self.initialize_logging()\n",
    "\n",
    "    def initialize_logging(self):\n",
    "\n",
    "        '''\n",
    "        Initiates logging to record information into the log file as specified in the configuration.\n",
    "        '''\n",
    "        logging.basicConfig(filename=self.configuration['log_path'], level=logging.INFO)\n",
    "        logging.info('Application initiated.')\n",
    "\n",
    "    def on_created(self, event):\n",
    "\n",
    "        '''\n",
    "        \n",
    "        This function is called when a new file is generated within the input directory.\n",
    "\n",
    "        It loads the data, processes it, makes predictions, saves the results and logs the process.\n",
    "\n",
    "        Args:\n",
    "            event (FileSystemEvent): Event representing file system changes.\n",
    "        '''\n",
    "\n",
    "        file_name = event.src_path\n",
    "        logging.info(f'Detected new file: {file_name}')\n",
    "\n",
    "        try:\n",
    "            \n",
    "            loaded_data = pd.read_csv(file_name)\n",
    "            processed_data = self.preprocess_data(loaded_data)\n",
    "            model_predictions = self.loaded_model.predict(processed_data)\n",
    "            updated_predictions = processed_data.copy()\n",
    "            updated_predictions['prediction'] = model_predictions\n",
    "\n",
    "            # Storing outcomes in the output directory using the identical filename.\n",
    "            updated_predictions.to_csv(os.path.join(self.configuration['output_directory'], os.path.basename(file_name)))\n",
    "\n",
    "            # Creating visual representations of sensor data if indicated in the configuration.\n",
    "            for sensor in self.configuration['sensors_to_draw']:\n",
    "                self.visualize_sensor(updated_predictions, sensor)\n",
    "\n",
    "            logging.info('File processing completed.')\n",
    "        except Exception as ex:\n",
    "            logging.error(f'Error while processing file: {ex}')\n",
    "\n",
    "    def preprocess_data(self, data):\n",
    "\n",
    "        '''\n",
    "        This function converts the data into a suitable format for use by the model.\n",
    "\n",
    "        It populates any absent values with the average of the column and normalizes the data utilizing the scaler that was preloaded.\n",
    "\n",
    "        Args:\n",
    "            data (pandas.DataFrame): Raw data to be processed.\n",
    "\n",
    "        Returns:\n",
    "            data (pandas.DataFrame): Data has been prepared and is now suitable for input to the model..\n",
    "        '''\n",
    "\n",
    "        data = data.fillna(data.mean())\n",
    "        data = pd.DataFrame(self.data_scaler.transform(data), columns=data.columns)\n",
    "        return data\n",
    "\n",
    "    def visualize_sensor(self, data, sensor):\n",
    "        '''\n",
    "        This function visualizes the sensor data and saves it as a file.\n",
    "\n",
    "        It plots the specified sensor's data and saves the plot in the image directory specified in the configuration.\n",
    "\n",
    "        Args:\n",
    "            data (pandas.DataFrame): Data containing the sensor values.\n",
    "            sensor (str): Sensor to be visualized.\n",
    "        '''\n",
    "\n",
    "        figure, axis = plt.subplots()\n",
    "        data[sensor].plot(ax=axis)\n",
    "        # Saving the plot to a file\n",
    "        figure.savefig(os.path.join(self.configuration['image_directory'], f'{sensor}.png'))\n",
    "\n",
    "\n",
    "def parse_config(config_file):\n",
    "\n",
    "    '''\n",
    "    This function parses the configuration from the given JSON file.\n",
    "\n",
    "    Args:\n",
    "        config_file (str): Path to the JSON configuration file.\n",
    "\n",
    "    Returns:\n",
    "        configuration (dict): Configuration parameters as a dictionary.\n",
    "    '''\n",
    "\n",
    "    with open(config_file) as file:\n",
    "        return json.load(file)\n",
    "\n",
    "\n",
    "def application():\n",
    "\n",
    "    '''\n",
    "    Main application function.\n",
    "\n",
    "    It reads the configuration, sets up the FileProcessor, and commences the file system observer to monitor for new files.\n",
    "    This process continues until an interrupt signal is detected.\n",
    "    '''\n",
    "\n",
    "    # Loading configuration from JSON file\n",
    "    config_data = parse_config('config.json')\n",
    "    file_observer = FileProcessor(config_data)\n",
    "\n",
    "    # Configuring a file event observer to monitor the input directory.\n",
    "    file_event_observer = Observer()\n",
    "    file_event_observer.schedule(file_observer, config_data['input_directory'], recursive=False)\n",
    "    file_event_observer.start()\n",
    "\n",
    "    # Continue program execution until an interrupt signal is encountered.\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        file_event_observer.stop()\n",
    "\n",
    "    file_event_observer.join()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    application()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
